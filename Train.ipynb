{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Custom Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset using a port of the https://github.com/matterport/Mask_RCNN project to PyTorch (PyTorch project repo: https://github.com/multimodallearning/pytorch-mask-rcnn).\n",
    "\n",
    "Prerequisite:  run the Setup_and_Demo.ipynb first to build two important extensions required for this project.\n",
    "\n",
    "Further reading on Mask RCNN and very similar approach (associated with the codebase from which this is the PyTorch port):  https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46\n",
    "\n",
    "\n",
    "Data links found below in the Fish Dataset section.\n",
    "\n",
    "To add data use the VGG Image Annotator tool found here for web use or download:  http://www.robots.ox.ac.uk/~vgg/software/via/.\n",
    "\n",
    "This notebook is roughly based on the https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb from the matterport MaskRCNN with TensorFlow.\n",
    "\n",
    "**Docker Notes**\n",
    "\n",
    "* Use `docker commit` to take a container and save as an image to the running docker instance.  Then use `docker save` to save it to an archived file for backup purposes.\n",
    "* Avoid updating Docker while working on a project such as this or changing the \"Disk image max size\" in Settings as this could erase the images and containers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0.dev20181102'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f6ae9c9c4191>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodellib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/bin/github/pytorch-mask-rcnn-samples/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnms_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mroialign\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_align\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop_and_resize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCropAndResizeFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/bin/github/pytorch-mask-rcnn-samples/nms/nms_wrapper.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpth_nms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpth_nms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/bin/github/pytorch-mask-rcnn-samples/nms/pth_nms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpth_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/bin/github/pytorch-mask-rcnn-samples/nms/_ext/nms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_wrap_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_nms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mffi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.cpp'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import PIL\n",
    "from PIL import Image, ImageOps\n",
    "import skimage\n",
    "from skimage import draw\n",
    "import h5py\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "from torchvision import datasets\n",
    "import pycocotools\n",
    "\n",
    "# from azure.storage import CloudStorageAccount\n",
    "# from azure.storage.blob import BlockBlobService\n",
    "\n",
    "# Root directory of the project\n",
    "TOP_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(TOP_DIR)  # To find local version of the library\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "import fish_pytorch_style\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory of images to run detection on\n",
    "DATA_DIR = os.path.join(ROOT_DIR, \"fish_pics\")\n",
    "\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Path to trained weights file\n",
    "# Download this file and place in the root of your\n",
    "# project (See README file for link to model)\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "This dataset required image files and their annotations (which are the polygon shapes drawn around fish).  Data used for this notebook can be generated by using images of an object of interest, here fish images from a web scrape.  The labels come from polygons created by using the [VGG Image Annotator](http://www.robots.ox.ac.uk/~vgg/software/via/).\n",
    "\n",
    "\n",
    "Example annotation:\n",
    "![VGG annotation](images/vgg_annotated_fish.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with getting images - get only as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(picking_files.loc[picking_files['seenfish_manual'] == True]['frame'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of using Azure storage SDK to access images in Blob Storage on Azure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blockblob_service = BlockBlobService(account_name='your accountname', account_key='your key') # <---- fill in and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blob_list = []\n",
    "# csv_file = ''\n",
    "# container_name = 'raw'\n",
    "# folder_name = 'LIS3B2-20170706-3'\n",
    "# generator = blockblob_service.list_blobs(container_name)\n",
    "\n",
    "# for blob in generator:\n",
    "#     # Get frames for a particular folder path\n",
    "#     if folder_name in blob.name and 'frame' in blob.name:\n",
    "#         blob_list.append(blob.name)\n",
    "#     # Get the list of pics with fish in them found in csv file in folder\n",
    "#     if folder_name in blob.name and 'result_fish' in blob.name:\n",
    "#         print(blob.name)\n",
    "#         csv_file = blob.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(os.path.join('fish_pics', 'next_batch')):\n",
    "#     os.makedirs(os.path.join('fish_pics', 'next_batch'))\n",
    "\n",
    "# # Download blobs (frames) (ALL, unless they are specified)\n",
    "# for blob_name in blob_list:\n",
    "#     blockblob_service.get_blob_to_path(container_name, blob_name, \\\n",
    "#                                        os.path.join('fish_pics', 'next_batch', \\\n",
    "#                                        blob_name.split('/')[blob_name.count('/')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob(os.path.join('fish_pics', 'train', 'frame_*.jpg'))\n",
    "n_images = len(images)\n",
    "w=20\n",
    "h=20\n",
    "fig=plt.figure(figsize=(15, 15))\n",
    "columns = 5\n",
    "rows = 5\n",
    "for i in range(1, columns*rows + 1):\n",
    "    if (i-1) == n_images:\n",
    "        break\n",
    "    img = plt.imread(images[i-1])\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls fish_pics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "def equalize(im):\n",
    "    h = im.convert(\"L\").histogram()\n",
    "    lut = []\n",
    "    for b in range(0, len(h), 256):\n",
    "        # step size\n",
    "        step = reduce(operator.add, h[b:b+256]) / 255\n",
    "        # create equalization lookup table\n",
    "        n = 0\n",
    "        for i in range(256):\n",
    "            lut.append(n / step)\n",
    "            n = n + h[i+b]\n",
    "    # map image through lookup table\n",
    "    return im.point(lut*im.layers)\n",
    "\n",
    "def resize(im, desired_size):\n",
    "    old_size = im.size  # old_size[0] is in (width, height) format\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    delta_w = desired_size - new_size[0]\n",
    "    delta_h = desired_size - new_size[1]\n",
    "    padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
    "    new_im = ImageOps.expand(im, padding)\n",
    "    new_im = ImageOps.fit(new_im, (int(old_size[0]/2), int(old_size[1]/2)))\n",
    "    return new_im\n",
    "\n",
    "import operator\n",
    "\n",
    "def histogram_strech(im):\n",
    "    h = im.convert(\"L\").histogram()\n",
    "    lut = []\n",
    "    for b in range(0, len(h), 256):\n",
    "        # step size\n",
    "        step = reduce(operator.add, h[b:b+256]) / 255\n",
    "        # create equalization lookup table\n",
    "        n = 0\n",
    "        for i in range(256):\n",
    "            lut.append(n / step)\n",
    "            n = n + h[i+b]\n",
    "    # map image through lookup table\n",
    "    return im.point(lut*im.layers)\n",
    "\n",
    "from numpy import *\n",
    "\n",
    "def denoise(im,U_init,tolerance=0.1,tau=0.125,tv_weight=100):\n",
    "    \"\"\" An implementation of the Rudin-Osher-Fatemi (ROF) denoising model\n",
    "    using the numerical procedure presented in eq (11) A. Chambolle (2005).\n",
    "\n",
    "    Input: noisy input image (grayscale), initial guess for U, weight of\n",
    "    the TV-regularizing term, steplength, tolerance for stop criterion.\n",
    "\n",
    "    Output: denoised and detextured image, texture residual. \"\"\"\n",
    "\n",
    "    m,n = im.shape # size of noisy image\n",
    "\n",
    "    # initialize\n",
    "    U = U_init\n",
    "    Px = im # x-component to the dual field\n",
    "    Py = im # y-component of the dual field\n",
    "    error = 1\n",
    "\n",
    "    while (error > tolerance):\n",
    "        Uold = U\n",
    "\n",
    "        # gradient of primal variable\n",
    "        GradUx = roll(U,-1,axis=1)-U # x-component of U's gradient\n",
    "        GradUy = roll(U,-1,axis=0)-U # y-component of U's gradient\n",
    "\n",
    "        # update the dual varible\n",
    "        PxNew = Px + (tau/tv_weight)*GradUx\n",
    "        PyNew = Py + (tau/tv_weight)*GradUy\n",
    "        NormNew = maximum(1,sqrt(PxNew**2+PyNew**2))\n",
    "\n",
    "        Px = PxNew/NormNew # update of x-component (dual)\n",
    "        Py = PyNew/NormNew # update of y-component (dual)\n",
    "\n",
    "        # update the primal variable\n",
    "        RxPx = roll(Px,1,axis=1) # right x-translation of x-component\n",
    "        RyPy = roll(Py,1,axis=0) # right y-translation of y-component\n",
    "\n",
    "        DivP = (Px-RxPx)+(Py-RyPy) # divergence of the dual field.\n",
    "        U = im + tv_weight*DivP # update of the primal variable\n",
    "\n",
    "        # update of error\n",
    "        error = linalg.norm(U-Uold)/sqrt(n*m);\n",
    "\n",
    "    return U,im-U # denoised image and texture residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = glob.glob(os.path.join('fish_pics', '*.jpg'))\n",
    "# print(images)\n",
    "# w=10\n",
    "# h=10\n",
    "# fig=plt.figure(figsize=(15, 15))\n",
    "# columns = 4\n",
    "# rows = 2\n",
    "# for i in range(1, columns*rows + 1):\n",
    "#     img = Image.open(images[i-1])\n",
    "# #     img = ImageOps.equalize(img)\n",
    "# #     img = np.asarray(img)\n",
    "#     img = equalize(img)\n",
    "#     img = resize(img, 256)\n",
    "#     fig.add_subplot(rows, columns, i)\n",
    "#     plt.imshow(img)\n",
    "# #     plt.imsave(images[i-1].replace('.jpg', '_p.jpg'), np.asarray(img))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset Class\n",
    "\n",
    "This is a custom class extending the `Dataset` class in `utils.py`.  This specifically deals with the annotations exported as json polygons from the VGG Image Annotator.  Note, a mask is the space within the polygon representing the shape and area of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "#  Dataset\n",
    "############################################################\n",
    "\n",
    "class FishDataset(utils.Dataset):\n",
    "\n",
    "    def load_fish(self, dataset_dir, subset, region_data_json):\n",
    "        \"\"\"Load a subset of the fish dataset.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or val\n",
    "        \"\"\"\n",
    "        # Add classes. We have only one class to add.\n",
    "        self.add_class(\"fish\", 1, \"fish\")\n",
    "\n",
    "        # Train or validation dataset?\n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        # Load annotations\n",
    "        # VGG Image Annotator saves each image in the form:\n",
    "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
    "        #   'regions': {\n",
    "        #       '0': {\n",
    "        #           'region_attributes': {},\n",
    "        #           'shape_attributes': {\n",
    "        #               'all_points_x': [...],\n",
    "        #               'all_points_y': [...],\n",
    "        #               'name': 'polygon'}},\n",
    "        #       ... more regions ...\n",
    "        #   },\n",
    "        #   'size': 100202\n",
    "        # }\n",
    "        # We mostly care about the x and y coordinates of each region\n",
    "        annotations = json.load(open(os.path.join(dataset_dir, region_data_json)))\n",
    "        annotations = list(annotations.values())  # don't need the dict keys\n",
    "\n",
    "        # The VIA tool saves images in the JSON even if they don't have any\n",
    "        # annotations. Skip unannotated images.\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "\n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            # Get the x, y coordinaets of points of the polygons that make up\n",
    "            # the outline of each object instance. There are stores in the\n",
    "            # shape_attributes (see json format above)\n",
    "            polygons = [r['shape_attributes'] for r in a['regions'].values()]\n",
    "\n",
    "            # load_mask() needs the image size to convert polygons to masks.\n",
    "            # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
    "            # the image. This is only managable since the dataset is tiny.\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            self.add_image(\n",
    "                \"fish\",\n",
    "                image_id=a['filename'],  # use file name as a unique image id\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                polygons=polygons)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a fish dataset image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"fish\":\n",
    "            print (\"Not a fish label\")\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        # Convert polygons to a bitmap mask of shape\n",
    "        # [height, width, instance_count]\n",
    "        info = self.image_info[image_id]\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n",
    "                        dtype=np.uint8)\n",
    "        for i, p in enumerate(info[\"polygons\"]):\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\n",
    "            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n",
    "            mask[rr, cc, i] = 1\n",
    "\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID only, we return an array of 1s\n",
    "        return mask, np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"fish\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Training and Validation Datasets\n",
    "\n",
    "For the images there are two sub folders inside the images (`fish_pics`) directory, namely: `train` and `val`.  These two subfolders have the images which correspond to labels (polygons here) contained in json file - the output of the VGG Annotator Tool (http://www.robots.ox.ac.uk/~vgg/software/via/via.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training dataset. Use the training set and 35K from the\n",
    "# validation set, as as in the Mask RCNN paper.\n",
    "dataset_train = FishDataset()\n",
    "dataset_train.load_fish(DATA_DIR, \"train\", \"via_region_data_train.json\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "dataset_val = FishDataset()\n",
    "dataset_val.load_fish(DATA_DIR, \"val\",  \"via_region_data_val.json\")\n",
    "dataset_val.prepare()\n",
    "\n",
    "# Original COCO class names:\n",
    "# class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n",
    "#                'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "#                'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n",
    "#                'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "#                'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n",
    "#                'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "#                'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "#                'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n",
    "#                'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "#                'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "#                'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n",
    "#                'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n",
    "#                'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "#                'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n",
    "#                'teddy bear', 'hair drier', 'toothbrush', 'fish']\n",
    "\n",
    "class_names = ['BG', 'fish']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FishConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy  dataset.\n",
    "    Derives from the base Config class and overrides some values.\n",
    "    \"\"\"\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 0\n",
    "    IMAGES_PER_GPU = 2\n",
    "    \n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"fish\"\n",
    "\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1\n",
    "    \n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 4\n",
    "\n",
    "    # Skip detections with < 90% confidence\n",
    "    DETECTION_MIN_CONFIDENCE = 0.90\n",
    "    \n",
    "    \n",
    "    COCO_MODEL_PATH = os.getcwd() + os.sep + 'mask_rcnn_coco.pth'\n",
    "    \n",
    "    VALIDATION_STEPS = 3\n",
    "    \n",
    "    BATCH_SIZE = 2\n",
    "    \n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # Necessary for docker immage to optimize memory usage best\n",
    "    NUM_WORKERS = 0\n",
    "    \n",
    "# Configurations\n",
    "config = FishConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Fish Model\n",
    "\n",
    "Note the \"Further reading\" link at the top to find out the details of this model.  For a quick summary the model architecture consists of the following modules:\n",
    "\n",
    "1.  Backbone\n",
    "  * Resnet CNN as a featurizer\n",
    "  * Feature Pyramid Network - refines the features to better represent objects at multiple scales.\n",
    "2.  Region Proposal Network - scans and finds areas that contain objects\n",
    "  * Output:\n",
    "    * Anchor class\n",
    "    * Bounding box refinement\n",
    "3.  Region of Interest Classifier (using ROI Align for ROI pooling) and Bounding Box Regressor\n",
    "  * Output\n",
    "    * Class (Background, BG, included)\n",
    "    * Bounding box refinement (further refinement)\n",
    "4.  Segmentation Mask Network (CNN)\n",
    "  * Output:  \"soft\" masks (floating point numbers instead of binary mask for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "Using the COCO pretrained weights file (or pretrained model on this data to refine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create model object.\n",
    "model = modellib.MaskRCNN(model_dir=MODEL_DIR, config=config)\n",
    "if config.GPU_COUNT > 0:\n",
    "    print('Cuda!', config.GPU_COUNT)\n",
    "    model = model.cuda()\n",
    "\n",
    "# Load weights trained on MS-COCO - all weights\n",
    "# model.load_state_dict(torch.load(COCO_MODEL_PATH))\n",
    "\n",
    "# Remove the penultimate layer of MS-COCO\n",
    "pretrained_state = torch.load(COCO_MODEL_PATH)\n",
    "model_state = model.state_dict()\n",
    "\n",
    "pretrained_state = { k:v for k,v in pretrained_state.items() if k in model_state and v.size() == model_state[k].size() }\n",
    "model_state.update(pretrained_state)\n",
    "model.load_state_dict(model_state)\n",
    "\n",
    "# # Use a pretrained model from this notebook\n",
    "# model.load_state_dict(torch.load(os.path.join('logs', 'fish20180429T0019', 'mask_rcnn_fish_0052.pth')))\n",
    "\n",
    "# # Remove the penultimate layer of xyz model\n",
    "# pretrained_state = torch.load(os.path.join('logs', 'fish20180429T0019', 'mask_rcnn_fish_0052.pth'))\n",
    "# model_state = model.state_dict()\n",
    "\n",
    "# pretrained_state = { k:v for k,v in pretrained_state.items() if k in model_state and v.size() == model_state[k].size() }\n",
    "# model_state.update(pretrained_state)\n",
    "# model.load_state_dict(model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers='all'` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# *** This training schedule is an example. Update to your needs ***\n",
    "\n",
    "# Training - Stage 1\n",
    "print(\"Training network heads\")\n",
    "model.train_model(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=40,\n",
    "            layers='heads')\n",
    "\n",
    "# Training - Stage 2\n",
    "# Finetune layers from ResNet stage 4 and up\n",
    "print(\"Fine tune Resnet stage 4 and up\")\n",
    "model.train_model(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=60,\n",
    "            layers='4+')\n",
    "\n",
    "# Training - Stage 3\n",
    "# Fine tune all layers\n",
    "print(\"Fine tune all layers\")\n",
    "model.train_model(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=70,\n",
    "            layers='all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of loading a previously trained model checkpoint\n",
    "# model = modellib.MaskRCNN(model_dir=MODEL_DIR, config=config)\n",
    "# model.load_state_dict(torch.load(os.path.join('logs', 'fish20180430T1611', 'mask_rcnn_fish_0070.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls fish_pics/test/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_file = glob.glob(os.path.join('fish_pics' , 'test', '*.jpg'))[0]\n",
    "# pic_file = os.path.join('fish_pics', '35301600622_8bc287ac45_o.jpg')\n",
    "# pic_file = os.path.join('fish_pics', 'test', 'frame_31050.jpg')\n",
    "# pic_file = glob.glob('images/*.*')[0]\n",
    "\n",
    "\n",
    "# im = array(Image.open(pic_file).convert('L'))\n",
    "# original_image,T = denoise(im,im)\n",
    "# print(original_image.shape)\n",
    "# original_image = original_image.astype(int)\n",
    "# original_image = skimage.color.gray2rgb(original_image)\n",
    "# print(original_image.shape)\n",
    "\n",
    "# img = histogram_strech(Image.open(pic_file))\n",
    "# original_image = np.asarray(img)\n",
    "\n",
    "original_image = plt.imread(pic_file)\n",
    "plt.imshow(original_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image])\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_file = glob.glob('images/*.*')[0]\n",
    "\n",
    "original_image = plt.imread(pic_file)\n",
    "plt.imshow(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.detect([original_image])\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(FishConfig):\n",
    "    GPU_COUNT = 0\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".pth file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "# log(\"original_image\", original_image)\n",
    "# log(\"image_meta\", image_meta)\n",
    "# log(\"gt_class_id\", gt_class_id)\n",
    "# log(\"gt_bbox\", gt_bbox)\n",
    "# log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mean Average Precision (mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 5)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image])\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (sys)",
   "language": "python",
   "name": "py36sys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
